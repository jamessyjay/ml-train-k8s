apiVersion: v1  # v1 is the API version for Service
# rendezvous service for torchrun (used for distributed training over multiple nodes)
# https://docs.pytorch.org/docs/stable/elastic/rendezvous.html
kind: Service
metadata:
  name: train-rdzv
  namespace: train
spec:
  clusterIP: None
  selector: 
    app: train-llm
  ports:
    - {name: rdzv, port: 29500, targetPort: 29500}

---
apiVersion: apps/v1 # apps/v1 is the API version for StatefulSet
kind: StatefulSet   # stateful set for distributed training
metadata:
  name: train-llm
  namespace: train
spec:
  serviceName: train-rdzv   # rendezvous service
  replicas: 4  # number of nodes
  # parallel pod creation
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app: train-llm
  template:
    metadata:
      labels:
        # label for pod selection
        app: train-llm
    spec:
      restartPolicy: Always
      # https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-terminating-with-grace
      terminationGracePeriodSeconds: 120
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            # the less the weight, the less the pods are spread
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    # spread only pods with the same label
                    app: train-llm
                topologyKey: kubernetes.io/hostname
      containers:
        - name: trainer
          # docker image with pytorch and transformers
          image: ghcr.io/jamessyjay/gpu-cluster-acceptance:train-smoke
          imagePullPolicy: IfNotPresent
          command: ["bash","-lc"]
          args:
            - |
              set -euo pipefail
              export RANK=${HOSTNAME##*-}              # pod index as rank
              export WORLD_SIZE=$(($NNODES * $NPROC_PER_NODE))
              export EPOCHS=1       # number of epochs
              export BATCH=2        # per-GPU batch size (OOM-safe default)
              export SEQ_LEN=3072   # training sequence length (OOM-safe default)
              export GRAD_ACCUM=40  # gradient accumulation to keep tokens/step

              # Recommended ENV
              export CUDA_DEVICE_MAX_CONNECTIONS=1
              torchrun --nnodes=$NNODES --node_rank=$RANK --nproc_per_node=$NPROC_PER_NODE \
                --rdzv_backend=c10d --rdzv_endpoint=${MASTER_ADDR}:${MASTER_PORT} --rdzv_conf timeout=300 \
                /app/src/train_framework.py \
                --model Qwen/Qwen2.5-3B-Instruct --data /data/instruct.jsonl \
                --output /out/run-$(date +%Y%m%d-%H%M) \
                --lora --bf16 --epochs $EPOCHS --seq-len $SEQ_LEN --batch $BATCH --grad-accum $GRAD_ACCUM \
                --log-level DEBUG --log-batches all --report-json /out/report.json \
                && sleep 10
          env:
            - { name: CLUSER_FQDN, value: train-rdzv.train.svc.cluster.local }
            - { name: HF_HOME, value: /mnt/filesystem-o2/.cache/hf }               # helps with memory allocation
            - { name: TORCH_CUDA_ALLOC_CONF, value: "expandable_segments:True" }   # helps with memory allocation
            - { name: NCCL_IB_DISABLE, value: "1" }                                # no IB â†’ TCP
            - { name: NCCL_SOCKET_IFNAME, value: "eth0" }                          # your netif
            - { name: NCCL_ALGO, value: "Ring" }                                   # try Ring/Tree
            - { name: NCCL_NSOCKS_PERTHREAD, value: "8" }                          # helps with TCP over GPUs
            - { name: NCCL_SOCKET_NTHREADS, value: "8" }                           # helps with TCP over GPUs
            - { name: TORCH_NCCL_ASYNC_ERROR_HANDLING, value: "1" }                # helps with TCP over GPUs
            - { name: NCCL_DEBUG, value: "INFO" }                                  # helps with TCP over GPUs
            - { name: MASTER_ADDR, value: train-llm-0.${CLUSER_FQDN} }             # rendezvous service address (FQDN)
            - { name: MASTER_PORT, value: "29500" }                                # rendezvous service port
            - { name: NNODES, value: "4" }                                         # number of nodes
            - { name: NPROC_PER_NODE, value: "1" }                                 # number of processes per node
            - { name: OMP_NUM_THREADS, value: "16" }                               # number of threads per process
            - { name: HUGGINGFACE_HUB_TOKEN, valueFrom: { secretKeyRef: { name: hf, key: HUGGINGFACE_HUB_TOKEN } } }
          ports:
            - { name: rdzv, containerPort: 29500 }
          resources:
            # current node limits
            limits:
              nvidia.com/gpu: 1
              cpu: "16"
              memory: 128Gi
            requests:
              nvidia.com/gpu: 1
              cpu: "12"
              memory: 96Gi
          volumeMounts:
            - { name: shared, mountPath: /data, subPath: datasets }
            - { name: shared, mountPath: /out, subPath: outputs }
            - { name: shared, mountPath: /models, subPath: models }

      volumes:
        - name: shared
          hostPath:
            path: /mnt/filesystem-o2
            type: Directory