apiVersion: v1
# rendezvous service for torchrun (used for distributed smoke test over multiple nodes)
kind: Service
metadata:
  name: train-smoke-rdzv
  namespace: train
spec:
  clusterIP: None
  selector:
    app: train-smoke
  ports:
    - { name: rdzv, port: 29500, targetPort: 29500 }

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: train-smoke
  namespace: train
spec:
  serviceName: train-smoke-rdzv
  replicas: 4
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app: train-smoke
  template:
    metadata:
      labels:
        app: train-smoke
    spec:
      restartPolicy: Always
      terminationGracePeriodSeconds: 120
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: train-smoke
                topologyKey: kubernetes.io/hostname
      containers:
        - name: trainer
          image: ghcr.io/jamessyjay/gpu-cluster-acceptance:train-smoke
          imagePullPolicy: IfNotPresent
          command: ["bash", "-lc"]
          args:
            - |
              set -euo pipefail
              export EPOCHS=2

              # Recommended ENV
              export CUDA_DEVICE_MAX_CONNECTIONS=1

              torchrun --nnodes=4:64 --nproc_per_node=$NPROC_PER_NODE \
                --rdzv_backend=c10d --rdzv_endpoint=${MASTER_ADDR}:${MASTER_PORT} \
                /app/src/train.py \
                --epochs $EPOCHS --samples $SAMPLES --batch $BATCH --features $FEATURES --classes $CLASSES --lr $LR \
                && sleep 5
          env:
            - { name: NODE_NAME, valueFrom: { fieldRef: { fieldPath: spec.nodeName } } }
            - { name: TORCH_CUDA_ALLOC_CONF, value: "expandable_segments:True" }
            - { name: NCCL_IB_DISABLE, value: "1" }
            - { name: NCCL_SOCKET_IFNAME, value: "eth0" }
            - { name: NCCL_ALGO, value: "Ring" }
            - { name: NCCL_NSOCKS_PERTHREAD, value: "8" }
            - { name: NCCL_SOCKET_NTHREADS, value: "8" }
            - { name: TORCH_NCCL_ASYNC_ERROR_HANDLING, value: "1" }
            - { name: NCCL_DEBUG, value: "INFO" }
            - { name: MASTER_ADDR, value: train-smoke-0.train-smoke-rdzv }
            - { name: MASTER_PORT, value: "29500" }
            - { name: NPROC_PER_NODE, value: "1" }
            - { name: OMP_NUM_THREADS, value: "16" }
            - { name: SAMPLES, value: "4096" }  # number of samples for training
            - { name: BATCH, value: "256" }     # batch size per GPU
            - { name: FEATURES, value: "128" }   # features per sample during the training
            - { name: CLASSES, value: "4" }      # classes per sample during the training
            - { name: LR, value: "0.003" }      # learning rate
          ports:
            - { name: rdzv, containerPort: 29500 }
          resources:
            limits:
              nvidia.com/gpu: 1
              cpu: "16"
              memory: 64Gi
            requests:
              nvidia.com/gpu: 1
              cpu: "8"
              memory: 32Gi
          volumeMounts:
            - { name: shared, mountPath: /app/src, subPath: code/src }  # code directory
            - { name: shared, mountPath: /out, subPath: outputs }   # report directory
            - { name: dshm, mountPath: /dev/shm }   # for PyTorch
      volumes:
        - name: shared
          hostPath:
            path: /mnt/filesystem-o2
            type: Directory
        - name: dshm
          emptyDir:
            medium: Memory
